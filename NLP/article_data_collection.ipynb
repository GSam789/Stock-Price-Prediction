{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ctori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import twython\n",
    "import requests\n",
    "import nltk\n",
    "import warnings\n",
    "from newspaper import Article\n",
    "from htmldate import find_date\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(company, page_number):\n",
    "    base_url = \"https://investing.com/equities\"\n",
    "    url = f\"{base_url}/{company}/{page_number}\"\n",
    "    # print(f\"URL - {url}\")\n",
    "\n",
    "    try:\n",
    "        # print(\"Inside try block\")\n",
    "        # Fetch the URL\n",
    "        # print(\"Before get url\")\n",
    "        driver.get(url)\n",
    "        # print(\"After get url\")\n",
    "\n",
    "        # Variables for tracking scroll position\n",
    "        old_scroll_position = 0\n",
    "        new_scroll_position = None\n",
    "\n",
    "        # print(\"Before new!=old \")\n",
    "        # Scroll to the bottom of the page until the scroll position stops changing\n",
    "        while new_scroll_position != old_scroll_position:\n",
    "            old_scroll_position = driver.execute_script(\n",
    "                \"return (window.pageYOffset !== undefined) ? \"\n",
    "                \"window.pageYOffset : (document.documentElement || \"\n",
    "                \"document.body.parentNode || document.body);\"\n",
    "            )\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\n",
    "                \"var scrollingElement = (document.scrollingElement || \"\n",
    "                \"document.body);scrollingElement.scrollTop = \"\n",
    "                \"scrollingElement.scrollHeight;\"\n",
    "            )\n",
    "            new_scroll_position = driver.execute_script(\n",
    "                \"return (window.pageYOffset !== undefined) ? \"\n",
    "                \"window.pageYOffset : (document.documentElement || \"\n",
    "                \"document.body.parentNode || document.body);\"\n",
    "            )\n",
    "        # print(\"After new !=old\")\n",
    "\n",
    "        # List to store cleaned links\n",
    "        cleaned_links = []\n",
    "\n",
    "        # Find the element containing article links\n",
    "        # print(\"Before div_element\")\n",
    "        article_div_element = driver.find_element(By.CLASS_NAME, \"mb-4\")\n",
    "        # print(\"After div_element\")\n",
    "\n",
    "        # Extract all anchor elements within the article div\n",
    "        # print(\"Before article links\")\n",
    "        article_links = article_div_element.find_elements(By.TAG_NAME, \"a\")\n",
    "        # print(\"After article links\")\n",
    "\n",
    "\n",
    "        # Iterate through the links and filter out unwanted ones\n",
    "        # print(\"Before article link for loop\")\n",
    "        for article_link in article_links:\n",
    "            partial_link = article_link.get_attribute(\"href\")\n",
    "            if partial_link:\n",
    "                if \"https\" in partial_link and \"comments\" not in partial_link:\n",
    "                    cleaned_links.append(partial_link)\n",
    "                elif partial_link.startswith(\"/\") and \"comments\" not in partial_link:\n",
    "                    cleaned_links.append(f\"{base_url}{partial_link}\")\n",
    "        # print(\"After article link for loop\")\n",
    "        return np.unique(cleaned_links)\n",
    "\n",
    "    finally:\n",
    "        # Close the browser window\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results = ['https://www.investing.com/analysis/3-beatendown-stocks-poised-for-rebound-in-2024-200643983'\n",
      " 'https://www.investing.com/analysis/3-tech-stocks-to-buy-as-fed-pivot-odds-soar-on-peak-inflation-bets-200643620'\n",
      " 'https://www.investing.com/analysis/apple-can-the-stock-continue-to-go-higher-200643930'\n",
      " 'https://www.investing.com/analysis/could-a-strong-us-dollar-curb-rise-of-bitcoin-nasdaq-200643572'\n",
      " 'https://www.investing.com/analysis/is-warren-buffett-bracing-for-a-market-correction-200643837'\n",
      " 'https://www.investing.com/analysis/microsoft-may-dethrone-apple-as-the-most-valuable-stock-soon-200643877'\n",
      " 'https://www.investing.com/analysis/sp-500-5-reasons-to-still-expect-a-yearend-rally-200643540'\n",
      " 'https://www.investing.com/analysis/stocks-week-ahead-nvidia-earnings-pose-crucial-test-for-yearend-rally-hopes-200643722'\n",
      " 'https://www.investing.com/analysis/unlock-prograde-portfolio-management-for-sustainable-longterm-returns-200643696'\n",
      " 'https://www.investing.com/analysis/will-us-stocks-lead-global-markets-again-in-2024-big-tech-has-the-answer-200643909'\n",
      " 'https://www.investing.com/members/contributors/200047220'\n",
      " 'https://www.investing.com/members/contributors/201248628'\n",
      " 'https://www.investing.com/members/contributors/202100653'\n",
      " 'https://www.investing.com/members/contributors/202934570'\n",
      " 'https://www.investing.com/members/contributors/204989407'\n",
      " 'https://www.investing.com/members/contributors/211180339'\n",
      " 'https://www.investing.com/members/contributors/231462125'\n",
      " 'https://www.investing.com/members/contributors/237365277'\n",
      " 'https://www.investing.com/members/contributors/43121']\n"
     ]
    }
   ],
   "source": [
    "companies = {\"apple\":\"apple-computer-inc-opinion\", \"microsoft\":\"microsoft-corp-opinion\", \"amazon\":\"amazon-com-inc-opinion\"}\n",
    "# companies = {\"apple\":\"apple-computer-inc-opinion\"}\n",
    "\n",
    "article_urls = {\"apple\":[], \"microsoft\":[], \"amazon\":[]}\n",
    "for company in companies:\n",
    "    for page in range(1,100):\n",
    "        results = get_article_links(companies[company], page)\n",
    "        # print(f\"Results = {results}\")\n",
    "        article_urls[company].extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "input_tickers = ['APPL', 'MSFT', 'AMZN']\n",
    "\n",
    "# Dictionary to store article sentiments for each ticker\n",
    "ticker_sentiments = {}\n",
    "\n",
    "# Mapping between tickers and company names\n",
    "ticker_company_name_data = {'APPL': \"apple\", 'MSFT': \"microsoft\", 'AMZN': \"amazon\"}\n",
    "\n",
    "# Create a DataFrame to populate while iterating\n",
    "for input_ticker in input_tickers:\n",
    "    # Initialize an empty DataFrame for each ticker\n",
    "    ticker_sentiments[input_ticker] = pd.DataFrame({\n",
    "        'ticker': [],\n",
    "        'publish_date': [],\n",
    "        'title': [],\n",
    "        'body_text': [],\n",
    "        'url': [],\n",
    "        'neg': [],\n",
    "        'neu': [],\n",
    "        'pos': [],\n",
    "        'compound': []\n",
    "    })\n",
    "\n",
    "    # Loop over all the articles for the current ticker\n",
    "    for link in article_urls[ticker_company_name_data[input_ticker]]:\n",
    "        article = Article(link)\n",
    "        article.download()\n",
    "\n",
    "        try:\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while parsing article: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize sentiment analyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "        # Get positive, negative, neutral, and compound scores\n",
    "        polarity = sid.polarity_scores(text)\n",
    "\n",
    "        # Create a dictionary with article information and sentiment scores\n",
    "        article_data = {\n",
    "            'ticker': input_ticker,\n",
    "            'publish_date': find_date(link),\n",
    "            'title': article.title,\n",
    "            'body_text': article.text,\n",
    "            'url': link\n",
    "        }\n",
    "\n",
    "        # Update the dictionary with sentiment scores\n",
    "        article_data.update(polarity)\n",
    "\n",
    "        # Concatenate the new data to the existing DataFrame for the current ticker\n",
    "        ticker_sentiments[input_ticker] = pd.concat([ticker_sentiments[input_ticker], pd.DataFrame(article_data, index=[0])])\n",
    "\n",
    "        # Reset the index for the DataFrame\n",
    "        ticker_sentiments[input_ticker].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame \n",
    "for ticker in input_tickers:\n",
    "    ticker_sentiments[ticker].to_pickle(f\"Raw Data/pickle/{ticker}_sentiments_data.pkl\")\n",
    "    ticker_sentiments[ticker].to_csv(f\"Raw Data/csv/{ticker}_sentiments_data.csv\", sep=',', encoding='utf-8', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
